{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stu83\\anaconda3\\envs\\dacon\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = './target_csv/'\n",
    "csv_name = 'target_protein_wiki'\n",
    "df = pd.read_csv(csv_dir + csv_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stu83\\anaconda3\\envs\\dacon\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 설정 (예: Sentence-BERT 사용)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_content(url):\n",
    "    \"\"\" 주어진 Wikipedia URL에서 내용을 크롤링하여 텍스트를 반환합니다. \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ' '.join([para.text for para in paragraphs])\n",
    "        return content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, llm_chain):\n",
    "    \"\"\"주어진 텍스트를 LLM을 통해 요약합니다.\"\"\"\n",
    "    try:\n",
    "        return llm_chain.invoke(text).content\n",
    "    except Exception as e:\n",
    "        return f\"Error during summarization: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 설정\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "template = \"\"\"\n",
    "Please remove any unnecessary phrases (e.g., 'citation needed', '[who?]', '[when?]' etc.) from the text below and retain only the core content. Clean up any parts that disrupt the flow of the text. However, do not remove any identifiers (e.g., gene or protein IDs).\n",
    "Text: {text}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt 설정\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # LLM이 처리할 입력 변수 이름\n",
    "    template=template\n",
    ")# LLM에게 줄 실제 프롬프트\n",
    "\n",
    "# LLMChain 생성\n",
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['Wiki'] != 'FALSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stu83\\AppData\\Local\\Temp\\ipykernel_1560\\254421582.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Wiki_documents'] = df_filtered['Wiki'].apply(\n"
     ]
    }
   ],
   "source": [
    "df_filtered['Wiki_documents'] = df_filtered['Wiki'].apply(\n",
    "    lambda x: summarize_text(get_wiki_content(x), llm_chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stu83\\AppData\\Local\\Temp\\ipykernel_1560\\3387006656.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['embedding'] = df_filtered['Wiki_documents'].apply(\n"
     ]
    }
   ],
   "source": [
    "df_filtered['embedding'] = df_filtered['Wiki_documents'].apply(\n",
    "    lambda text: None if text is None else embedding_model.encode(text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새 파일이 저장되었습니다: ./target_csv/protein_embedding_with_LLM.csv\n"
     ]
    }
   ],
   "source": [
    "# 결과를 새 CSV 파일로 저장\n",
    "output_file_path = './target_csv/protein_embedding_with_LLM.csv'\n",
    "df_filtered.to_csv(output_file_path, index=False)\n",
    "print(f\"새 파일이 저장되었습니다: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# csv_name = 'target_protein_wiki'\n",
    "# df = pd.read_csv(csv_name + '.csv')\n",
    "\n",
    "# def get_wiki_content(url):\n",
    "#     \"\"\" 주어진 Wikipedia URL에서 내용을 크롤링하여 텍스트를 반환합니다. \"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         paragraphs = soup.find_all('p')\n",
    "#         content = ' '.join([para.text for para in paragraphs])\n",
    "#         return content.strip()\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {str(e)}\"\n",
    "\n",
    "# # 새로운 컬럼 \"Wiki_documents\" 생성\n",
    "# df['Wiki_documents'] = df['Wiki'].apply(lambda x: None if x == \"FALSE\" else get_wiki_content(x))\n",
    "\n",
    "# # 결과를 새 CSV 파일로 저장\n",
    "# output_file_path = csv_name + '_wiki_documents.csv'\n",
    "# df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# print(f\"새 파일이 저장되었습니다: {output_file_path}\")\n",
    "# None GPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
