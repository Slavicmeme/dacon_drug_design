{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stu83\\anaconda3\\envs\\dacon\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = 'target_protein_wiki'\n",
    "df = pd.read_csv(csv_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stu83\\anaconda3\\envs\\dacon\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 설정 (예: Sentence-BERT 사용)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_content(url):\n",
    "    \"\"\" 주어진 Wikipedia URL에서 내용을 크롤링하여 텍스트를 반환합니다. \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ' '.join([para.text for para in paragraphs])\n",
    "        return content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, llm_chain):\n",
    "    \"\"\"주어진 텍스트를 LLM을 통해 요약합니다.\"\"\"\n",
    "    try:\n",
    "        return llm_chain.invoke(text).content\n",
    "    except Exception as e:\n",
    "        return f\"Error during summarization: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 설정\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt 설정\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],  # LLM이 처리할 입력 변수 이름\n",
    "    template=\"Please summarize the following text: {text}\"  # LLM에게 줄 실제 프롬프트\n",
    ")\n",
    "\n",
    "# LLMChain 생성\n",
    "llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Wiki_documents\" 컬럼 생성 및 요약 처리\n",
    "df['Wiki_documents'] = df['Wiki'].apply(\n",
    "    lambda x: None if x == \"FALSE\" else summarize_text(get_wiki_content(x), llm_chain)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 생성 및 \"embedding\" 컬럼 추가\n",
    "df['embedding'] = df['Wiki_documents'].apply(\n",
    "    lambda text: None if text is None else embedding_model.encode(text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                 Target Name  \\\n",
      "0      Tyrosine-protein kinase receptor RET   \n",
      "1   Macrophage-stimulating protein receptor   \n",
      "2                 Cyclin-dependent kinase 7   \n",
      "3                 Serine-protein kinase ATM   \n",
      "4                         Plasma kallikrein   \n",
      "..                                      ...   \n",
      "56          Thymidine kinase, mitochondrial   \n",
      "57         Stem cell growth factor receptor   \n",
      "58              Tyrosine-protein kinase BRK   \n",
      "59     Serine/threonine-protein kinase AKT3   \n",
      "60            PI3-kinase p110-gamma subunit   \n",
      "\n",
      "                                                 Wiki  \\\n",
      "0                                               FALSE   \n",
      "1                 https://en.wikipedia.org/wiki/MST1R   \n",
      "2   https://en.wikipedia.org/wiki/Cyclin-dependent...   \n",
      "3                                               FALSE   \n",
      "4     https://en.wikipedia.org/wiki/Plasma_kallikrein   \n",
      "..                                                ...   \n",
      "56     https://en.wikipedia.org/wiki/Thymidine_kinase   \n",
      "57           https://en.wikipedia.org/wiki/KIT_(gene)   \n",
      "58                                              FALSE   \n",
      "59                 https://en.wikipedia.org/wiki/AKT3   \n",
      "60               https://en.wikipedia.org/wiki/PIK3CG   \n",
      "\n",
      "                                       Wiki_documents  \\\n",
      "0                                                None   \n",
      "1   The text discusses the macrophage-stimulating ...   \n",
      "2   Cyclin-dependent kinase 7 (CDK7) is an enzyme ...   \n",
      "3                                                None   \n",
      "4   Plasma kallikrein is an enzyme encoded by the ...   \n",
      "..                                                ...   \n",
      "56  Thymidine kinase is a crucial enzyme involved ...   \n",
      "57  The text discusses the proto-oncogene c-KIT, w...   \n",
      "58                                               None   \n",
      "59  The text discusses the AKT3 gene, which encode...   \n",
      "60  The text discusses the PIK3CG gene, which enco...   \n",
      "\n",
      "                                            embedding  \n",
      "0                                                None  \n",
      "1   [-0.115056455, -0.041425236, -0.0656224, -0.02...  \n",
      "2   [-0.082206905, 0.05118553, -0.043077152, -0.01...  \n",
      "3                                                None  \n",
      "4   [-0.0692261, -0.062000677, -0.05440799, 0.0092...  \n",
      "..                                                ...  \n",
      "56  [-0.0645995, 0.039275337, -0.090597294, -0.075...  \n",
      "57  [-0.053047895, 0.03331242, -0.09673739, -0.044...  \n",
      "58                                               None  \n",
      "59  [-0.12965903, -0.025715845, -0.085271865, 0.06...  \n",
      "60  [-0.11130193, 0.0045191883, -0.08489185, -0.03...  \n",
      "\n",
      "[61 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "# print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새 파일이 저장되었습니다: target_protein_wiki_wiki_documents_summarized.csv\n"
     ]
    }
   ],
   "source": [
    "# 결과를 새 CSV 파일로 저장\n",
    "output_file_path = csv_name + '_wiki_documents_summarized.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "print(f\"새 파일이 저장되었습니다: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# csv_name = 'target_protein_wiki'\n",
    "# df = pd.read_csv(csv_name + '.csv')\n",
    "\n",
    "# def get_wiki_content(url):\n",
    "#     \"\"\" 주어진 Wikipedia URL에서 내용을 크롤링하여 텍스트를 반환합니다. \"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         paragraphs = soup.find_all('p')\n",
    "#         content = ' '.join([para.text for para in paragraphs])\n",
    "#         return content.strip()\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {str(e)}\"\n",
    "\n",
    "# # 새로운 컬럼 \"Wiki_documents\" 생성\n",
    "# df['Wiki_documents'] = df['Wiki'].apply(lambda x: None if x == \"FALSE\" else get_wiki_content(x))\n",
    "\n",
    "# # 결과를 새 CSV 파일로 저장\n",
    "# output_file_path = csv_name + '_wiki_documents.csv'\n",
    "# df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# print(f\"새 파일이 저장되었습니다: {output_file_path}\")\n",
    "# None GPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
